{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "import pickle \n",
    "import seaborn as sns\n",
    "import os,glob\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Create data folder to store data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createFolder(folderame,path):\n",
    "    newFolder=f\"{path}\\{folderame}\"\n",
    "    if not os.path.exists(newFolder):\n",
    "        os.makedirs(newFolder)\n",
    "    return newFolder\n",
    "projectDir=os.getcwd()\n",
    "datadir=createFolder(\"Data\",projectDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Download our dataset to data folder by using request library\n",
    "Data set link= https://archive.ics.uci.edu/ml/machine-learning-databases/00366/AReM.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00366/AReM.zip'\n",
    "r = requests.get(url)\n",
    "with open(f\"{datadir}\\AReM.zip\", \"wb\") as code:\n",
    "    code.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Create our data folder to keep  Raw ,modified , merge and final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata=createFolder(\"Raw Data\",datadir)\n",
    "modifiedData=createFolder(\"Modified Data\",datadir)\n",
    "mergedata=createFolder(\"Merge Data\",datadir)\n",
    "finalData=createFolder(\"Final Data\",datadir)\n",
    "human_activitydata=f\"{finalData}\\human_activity.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's get raw data by using getRawData() method. this method will unzip downloaded file and will keep raw data inside Raw Data folder which we created above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully file extracted and stored inside C:\\Users\\Soumya Ranjan\\I N E U R O N P R O J E C TS\\ML class project\\Human Activity Prediction\\Data\\Raw Data path \n",
      "Final Data: is not  Zip file \n",
      "Merge Data: is not  Zip file \n",
      "Modified Data: is not  Zip file \n",
      "Raw Data: is not  Zip file \n"
     ]
    }
   ],
   "source": [
    "def getRawData():\n",
    "    zipFileName=os.listdir(datadir)\n",
    "    try:\n",
    "        for file in zipFileName:\n",
    "            if \".zip\" in file:\n",
    "                with zipfile.ZipFile(f\"{datadir}\\{file}\" , 'r') as zip_ref:\n",
    "                    zip_ref.extractall(rawdata)\n",
    "                    print(f\"Successfully file extracted and stored inside {rawdata} path \")\n",
    "                    \n",
    "            else:\n",
    "                print(f\"{file}: is not  Zip file \")\n",
    "    except Exception as e:\n",
    "        print(\"Something gone wrong please check\",e)\n",
    "\n",
    "getRawData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# getOnlyFolder() method will give name of the folder present in given dirictory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOnlyFolder(path):\n",
    "    allClassName=[]\n",
    "    for file in os.listdir(rawdata):\n",
    "        if '.' in file:\n",
    "            print()\n",
    "\n",
    "        else:\n",
    "            allClassName.append(file)\n",
    "            \n",
    "    return allClassName\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['bending1', 'bending2', 'cycling', 'lying', 'sitting', 'standing', 'walking']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getOnlyFolder(rawdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# addLebel() will help us to add lebel name as classname  to each data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here this method will add lebel name for all dataset and it will return new datasets\n",
    "def addlebel(data,lebelname):\n",
    "    data[\"lebel\"]=lebelname\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's get modified data by using getModifiedData() method. this method will get all dataset from rawdata folder and will keep modified data inside Modified Data folder which we created above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 485: expected 7 fields, saw 8\\n'\n",
      "b'Skipping line 485: expected 7 fields, saw 8\\n'\n"
     ]
    }
   ],
   "source": [
    "def getModifiedData():\n",
    "    rawClassfile=getOnlyFolder(rawdata)\n",
    "    for className in rawClassfile:\n",
    "        classdir=f\"{rawdata}\\{className}\"\n",
    "        allCSVFile=os.listdir(classdir)\n",
    "        for csvFile in allCSVFile:\n",
    "            csvfilepath=f\"{classdir}\\{csvFile}\"\n",
    "            modifiedcsvPath=f\"{createFolder(className,modifiedData)}\\{csvFile}\"\n",
    "            df=pd.read_csv(csvfilepath,skiprows=4,error_bad_lines=False)\n",
    "            df.drop(columns=['# Columns: time'], inplace = True)\n",
    "            df = addlebel(df,className)\n",
    "            df.to_csv(modifiedcsvPath)\n",
    "\n",
    "        \n",
    "getModifiedData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's get merge data by using mergeAllClasscsv() method. this method will get all dataset from modified folder and will keep merge data inside Merged Data folder which we created above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mergeAllClasscsv():\n",
    "    modifiedClassfile=getOnlyFolder(modifiedData)\n",
    "    for className in modifiedClassfile:\n",
    "        classdir=f\"{modifiedData}\\{className}\"\n",
    "        allCSVFile=os.listdir(classdir)\n",
    "        for file in allCSVFile:\n",
    "            path=f\"{classdir}\\{file}\"\n",
    "            all_csv=glob.glob(os.path.join(classdir,\"dataset*.csv\"))\n",
    "            df_from_each_csv = (pd.read_csv(f, sep=',',error_bad_lines=False) for f in all_csv)\n",
    "            df_merged= pd.concat(df_from_each_csv, ignore_index=True)\n",
    "            df_merged.to_csv(f\"{mergedata}\\{className}.csv\")\n",
    "\n",
    "        \n",
    "mergeAllClasscsv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# let's get final data by using getFinalData() method. this method will take  all dataset from merged data folder and will keep final data inside Final Data folder which we created above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully we created final data for further processing and data is present inside   C:\\Users\\Soumya Ranjan\\I N E U R O N P R O J E C TS\\ML class project\\Human Activity Prediction\\Data\\Final Data\\human_activity.csv\n"
     ]
    }
   ],
   "source": [
    "def getFinalData():\n",
    "    allClassCsvFile=os.listdir(mergedata)\n",
    "    all_csv=glob.glob(os.path.join(mergedata,\"*.csv\"))\n",
    "    df_from_each_csv = (pd.read_csv(f, sep=',',error_bad_lines=False) for f in all_csv)\n",
    "    df_merged= pd.concat(df_from_each_csv, ignore_index=True)\n",
    "    df_merged.to_csv(human_activitydata)\n",
    "    print(\"Successfully we created final data for further processing and data is present inside  \", human_activitydata)\n",
    "        \n",
    "getFinalData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(human_activitydata).head()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
